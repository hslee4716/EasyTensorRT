{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load_model, tokenizer\n",
    "from transformers import AutoModelForCausalLM\n",
    "from transformers.models.gpt_neox import GPTNeoXTokenizerFast\n",
    "from time import time\n",
    "\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"5\"\n",
    "os.environ[\"CUDA_MODULE_LOADING\"]=\"LAZY\"\n",
    "\n",
    "base_model =\"nlpai-lab/kullm-polyglot-12.8b-v2\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "base_model,\n",
    "torch_dtype=torch.float16,\n",
    "low_cpu_mem_usage=True,\n",
    ").cuda()\n",
    "\n",
    "tokenizer = GPTNeoXTokenizerFast.from_pretrained('nlpai-lab/kullm-polyglot-12.8b-v2', export=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "model.save_pretrained(\"/home/team_ai/JMJ/nlp_model/kullm_dev/HS/sav_bin2\")\n",
    "\n",
    "# after this, run optimum_cli export\n",
    "# $ optimum-cli export onnx --model <saved_bin_folder_pth> --task text-generation <folder_path_for_sav_ONNX>\n",
    "os.system(\"optimum-cli export onnx --model /home/team_ai/JMJ/nlp_model/kullm_dev/HS/sav_bin2 --task text-generation /home/team_ai/JMJ/nlp_model/kullm_dev/HS/sav_onnx2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_test\n",
    "token = tokenizer(\"black top big t-shirts billie eilish\", return_tensors=\"pt\")\n",
    "r = model(token.input_ids.cuda())\n",
    "output = tokenizer.batch_decode(np.argmax(r[0].detach().cpu().numpy(),2), skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "print(output)\n",
    "\n",
    "for i in range(20):\n",
    "    t = time()\n",
    "    token = tokenizer(\"black top big t-shirts billie eilish\", return_tensors=\"pt\")\n",
    "    r = model(token.input_ids.cuda())\n",
    "    output = tokenizer.batch_decode(np.argmax(r[0].detach().cpu().numpy(),2), skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "    print(time()-t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'PreTrainedTokenizerFast'. \n",
      "The class this function is called from is 'GPTNeoXTokenizerFast'.\n",
      "2023-09-05 06:03:47.234398815 [W:onnxruntime:Default, onnxruntime_pybind_state.cc:614 CreateExecutionProviderInstance] Failed to create TensorrtExecutionProvider. Please reference https://onnxruntime.ai/docs/execution-providers/TensorRT-ExecutionProvider.html#requirements to ensure all dependencies are met.\n",
      "2023-09-05 06:03:47.234435926 [W:onnxruntime:Default, onnxruntime_pybind_state.cc:640 CreateExecutionProviderInstance] Failed to create CUDAExecutionProvider. Please reference https://onnxruntime.ai/docs/execution-providers/CUDA-ExecutionProvider.html#requirements to ensure all dependencies are met.\n"
     ]
    }
   ],
   "source": [
    "# onnx_test\n",
    "from transformers.models.gpt_neox import GPTNeoXTokenizerFast\n",
    "from onnxruntime import InferenceSession\n",
    "import numpy as np\n",
    "providers =  ['CUDAExecutionProvider', 'CPUExecutionProvider']\n",
    "tokenizer = GPTNeoXTokenizerFast.from_pretrained('nlpai-lab/kullm-polyglot-12.8b-v2', export=True)\n",
    "session = InferenceSession(\"/home/team_ai/JMJ/nlp_model/kullm_dev/HS/sav_onnx2/decoder_model.onnx\", providers=providers)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abing.ea hut bopshirts,igb eilish b']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ONNX Runtime expects NumPy arrays as input\n",
    "inputs = tokenizer(\"black top big t-shirts billie eilish\", return_tensors=\"np\")\n",
    "outputs = session.run(output_names=[\"logits\"],input_feed=dict(inputs))\n",
    "output = tokenizer.batch_decode(np.argmax(outputs[0],2), skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP_training",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
